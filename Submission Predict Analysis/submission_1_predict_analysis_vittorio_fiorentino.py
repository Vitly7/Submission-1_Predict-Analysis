# -*- coding: utf-8 -*-
"""Submission 1_Predict Analysis_Vittorio Fiorentino.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SsUEQ10GOnQvsRPUoRZ3gDmiQNAhSOme

#Data Understanding

##Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install yfinance
import yfinance as yf

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor

"""##Load Dataset"""

df = yf.download("ADRO.JK", start="2020-01-01", end="2025-01-01")

# Ubah multi-level columns jadi single-level
df.columns = df.columns.get_level_values(0)


# Lihat hasilnya
df.head()

"""##EDA"""

df.info()

"""Data Berisi type data numerik"""

df.describe()

"""Terlihat data volume memiliki nilai minimum 0 yang seharusnya tidak ada.

###Drop Missing Value
"""

volume = (df.Volume == 0).sum()
print(volume)

df.loc[(df['Volume']==0)]

# Drop baris dengan nilai 'Volume' = 0
df = df.loc[(df[['Volume']]!=0).all(axis=1)]

# Cek ukuran data untuk memastikan baris sudah di-drop
df.shape

"""Penghapusan data yang bernilai 0"""

df.describe()

"""###Mengatasi Outliers"""

# Buat plot untuk semua kolom numerik
numerical_cols = ['Open', 'High', 'Low', 'Close', 'Volume']

plt.figure(figsize=(15, 8))
for i, col in enumerate(numerical_cols):
    plt.subplot(2, 3, i+1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

"""Hasil Visual BoxPlot memperlihatkan bahwa bagian Volume memiliki Outliers"""

# Hitung Q1, Q3, dan IQR khusus untuk kolom 'Volume'
Q1 = df['Volume'].quantile(0.25)
Q3 = df['Volume'].quantile(0.75)
IQR = Q3 - Q1

# Buat filter untuk mendeteksi data yang tidak outlier
filter_outliers = ~((df['Volume'] < (Q1 - 1.5 * IQR)) | (df['Volume'] > (Q3 + 1.5 * IQR)))

# Terapkan filter ke seluruh dataset
df_cleaned = df[filter_outliers]

# Cek ukuran dataset setelah outlier Volume dihapus
print(df_cleaned.shape)

"""Data Volume yang bernilai outlier dihapus agar data bisa di proses lebih lanjut.

###Univariate Analysis
"""

# Kolom numerik
numerical_cols = df_cleaned.select_dtypes(include=['int64', 'float64']).columns

# Kolom kategorikal (termasuk object dan datetime)
categorical_cols = df_cleaned.select_dtypes(include=['object', 'category', 'datetime']).columns

print("Fitur numerik:", list(numerical_cols))
print("Fitur kategorikal:", list(categorical_cols))

"""Karena data bertype numerik semua, sehingga tidak ada analisis Category

####Numerical Features
"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""###Multivariate Analysis

####Numerical Features
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_cols].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Dari hasil Correlation Matrix menunjukkan bahwa Volume jauh dari korelasi, sehingga akan dibuang."""

df.drop(['Volume'], inplace=True, axis=1)
df.head()

"""#Data Preparation

##Reduksi Dimensi Dengan PCA
"""

sns.pairplot(df[['Close','High','Low','Open']], plot_kws={"s": 4});

# Ambil fitur
features = ['Close', 'High', 'Low', 'Open']
X = df[features]

# Standardisasi
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA setelah standardisasi
pca = PCA(n_components=4, random_state=123)
princ_comp = pca.fit_transform(X_scaled)

pca.explained_variance_ratio_.round(5)

"""##Train Test Split"""

# Misalnya df adalah DataFrame kamu
X = df[['Open', 'High', 'Low']]  # fitur (tanpa 'Close')
y = df['Close']  # target

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.1,      # 10% data untuk pengujian
    random_state=123    # supaya split-nya konsisten
)

"""Split Data: 90:10

Fitur: Open, High, Low
Target: Close
"""

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""##Standarisasi"""

numerical_cols = ['Open', 'High', 'Low']
scaler = StandardScaler()
scaler.fit(X_train[numerical_cols])
X_train[numerical_cols] = scaler.transform(X_train.loc[:, numerical_cols])
X_train[numerical_cols].head()

X_train[numerical_cols].describe().round(4)

"""#Model Deployment"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""Analisis Model akan menggunakan tiga jenis model, yaitu:

KNN, RandomForest, Boosting

##KNN
"""

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""##Random Forest"""

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""##Metode Adaptive Boosting"""

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""##Scaling Pada Data Uji"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_cols] = scaler.transform(X_test[numerical_cols])

"""#Evaluasi Model"""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Penjelasan:

- Boosting	~5.4	~4.4	Overfitting terlihat → error train & test cukup besar tapi tidak seimbang. Boosting cenderung fit terlalu dalam ke data latih.
- KNN	~0.26	~0.22	Cukup stabil → train dan test error mirip → model tidak overfit atau underfit.
- RF	~0.04	~0.2	Sangat baik → performa sangat bagus di train dan cukup baik di test. Hampir tidak overfit.

##Uji Data
"""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Evaluasi Akurasi Prediksi:

Hasil Asli = 1336.7
- KNN	1339.1 	 ->	Cukup dekat
- RandomForest	1337.5	 ->	Paling akurat
- Boosting	1304.0	 ->	Jauh lebih rendah dari aslinya

###Kesimpulan:
- Semua model cukup baik kali ini, terutama Random Forest yang prediksinya hampir identik dengan nilai sebenarnya.

- Model Boosting cenderung underestimate (meremehkan nilai).

- KNN juga cukup akurat, hanya sedikit lebih tinggi.
"""

